# 项目总结

使用深度可分离卷积  inceptionV3改进Unet

深度可分离卷积相比于常规卷积可以减少参数，一定程度上提高速度。

inception网络的结构设计思想就是将**卷积通道相关和空间通道相关**进行分离。



![img](https://upload-images.jianshu.io/upload_images/9389734-04f3f32b4539f2f9.png?imageMogr2/auto-orient/strip|imageView2/2/w/344/format/webp)

![img](https://upload-images.jianshu.io/upload_images/9389734-66c332ef4bb033c3.png?imageMogr2/auto-orient/strip|imageView2/2/w/400/format/webp)

Figure 1 是一个典型的 Inception 模块，它先在通道相关性上利用 1×1 卷积将输入的 Feature Map 映射到几个维度比原来更小的空间上，相当于每个通道图乘上不同的因子做线性组合，再用 3×3 卷积这些小空间，对它的空间和通道相关性同时做映射。以第二个分支为例，假设 Input 是 28×28×192 的 Feature Maps，在通道相关性上利用 32 个 1×1×192 的卷积核做线性组合，得到 28×28×32 大小的 Feature Maps，再对这些 Feature Maps 做 256 个 3×3×32 的卷积，即联合映射所有维度的相关性，就得到 28×28×256 的 Feature Maps 结果。可以发现，这个结果其实跟直接卷积 256 个3×3×192 大小的卷积核是一样。也就是说，Inception 的假设认为用 32 个 1×1×192 和 256 个 3×3×32 的卷积核退耦级联的效果，与直接用 256个 3×3×192 卷积核等效。而两种方式的参数量则分别为32×1×1×192 + 256×3×3×32 = 79872 和 256×3×3×192 = 442368。



1*1卷积层实际上为不同的feature map特征进行线性叠加，即特征组合

先进行1*1卷积，在进行3\*3卷积，实际上是将原先既改变维度又改变尺寸的3\*3卷积分为两个步骤， 将**卷积通道相关和空间通道相关**进行分离。



Figure 2 是简化后的 Inception 模块（仅使用3×3卷积并去除 Avg pooling），基于 Figure 2 的简化模块可以将所有的 1×1 卷积核整合成一个大的 1×1 卷积，如将 3 组 32 个 1×1×192 的卷积核重组为 96个 1×1×192 的卷积核，后续再接3组 3×3卷积，3×3卷积的输入为前序输出的1/3（Figure 3）。Xception 论文进而提出在 Figure 3 的基础上，是否可以做出进一步的假设：通道相关性和空间相关性是完全可分的，由此得到 Figure 4 中的 “extreme” Inception。如 Figure 4 所示，先进行 1×1 的通道相关性卷积，后续接的 3×3 卷积的个数与 1×1 卷积的输出通道数相同。

Figure 4 中的 Inception 模块与本文的主角-深度可分离卷积就近乎相似了，Figure4先使用1*1卷积对channel进行解耦，但仍然存在两点区别：

> 1、深度可分离卷积先进行 channel-wise 的空间卷积，再进行1×1 的通道卷积，Inception则相反；
>  2、Inception中，每个操作后会有一个ReLU的非线性激活，而深度可分离卷积则没有。

