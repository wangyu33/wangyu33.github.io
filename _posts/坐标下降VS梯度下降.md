## 坐标下降VS梯度下降

**梯度下降**

在每次迭代更新时选择负梯度方向(最速下降的方向)进行一次更新.不断迭代直至到达我们的目标或者满意为止.

**坐标下降**

坐标下降法属于一种非梯度优化的方法，它在每步迭代中沿一个坐标的方向进行搜索，通过循环使用不同的坐标方法来达到目标函数的局部极小值。求导时只对一个维度(坐标轴方向)进行求导,而固定其它维度,这样每次只优化一个分量.
相比梯度下降法而言，坐标下降法不需要计算目标函数的梯度，在每步迭代中仅需求解一维搜索问题，所以对于某些复杂的问题计算较为简便。但如果目标函数不平滑的话，坐标下降法可能会陷入非驻点。为了加速收敛，可以采用一个适当的坐标系，例如通过主成分分析获得一个坐标间尽可能不相互关联的新坐标系（参考自适应坐标下降法）



## SGD,Momentum,Adagard,Adam原理

**SGD**

SGD   stochastic gradient descent

每一次迭代计算数据集的mini-batch的梯度,然后对参数进行跟新

Momentum参考了物理中动量的概念,前几次的梯度也会参与到当前的计算中,但是前几轮的梯度叠加在当前计算中会有一定的衰减。

Adagard在训练的过程中可以自动变更学习的速率,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比。

Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率,在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。



##  矩阵正定性的判断,Hessian矩阵正定性在梯度下降中的应用

若矩阵所有特征值均不小于0,则判定为半正定。若矩阵所有特征值均大于0,则判定为正定。在判断优化算法的可行性时Hessian矩阵的正定性起到了很大的作用,若Hessian正定,则函数的二阶偏导恒大于0,函数的变化率处于递增状态,在牛顿法等梯度下降的方法中,Hessian矩阵的正定性可以很容易的判断函数是否可收敛到局部或全局最优解。



## 交叉熵与信息熵

信息熵
$$
Entropy=-\sum_{i}P(i)log_2P(i)
$$
交叉熵函数是将信息熵对数中的P(i)换为预测值
$$
Entropy=-\sum_{i}P(i)log_2\bar{P}(i)
$$


## LR

通俗来讲，划分超平面函数为y=wx+b，逻辑回归本质上是**线性回归**，只是在特征到结果的映射中加入了一层逻辑函数g(z)，即先把特征线性求和，然后使用函数g(z)作为假设函数来预测。g(z)可以将连续值映射到0 和1。g(z)为sigmoid function.
$$
\begin{eqnarray*}
&&g(z)=\frac{1}{1+e^{-z}}\to
h_\theta(x)=g(\theta^{T}x)=\frac{1}{1+e^{-\theta^Tx}}\\
&&其导函数如下:\\
&&g^{'}(z)=g(z)(1-g(z))
\end{eqnarray*}
$$
逻辑回归用来分类0/1 问题，也就是预测结果属于0 或者1 的二值分类问题。这里假设了二值满足伯努利分布，也就是
$$
p(y|;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{(1-y)}
$$
取log似然使用随机梯度上升规则使其最大来迭代更新参数

其log似然函数为交叉熵



## 逻辑回归怎么实现多分类

根据每个类别都建立一个二分类器,本类别的样本标签定义为0,其它分类样本标签定义为1,则有多少个类别就构造多少个逻辑回归分类器





## SVM基本思想

通俗的讲，**支持向量机是一种两类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略（分割原则）便是间隔最大化，最终可转换为一个凸二次规划问题的求解**

1 它是针对线性可分情况进行分析，对于线性不可分情况，通过使用非线性映射算法将低维输入空间线性不可分的样本转化为高维特征空间使其可分，从而使得高维特征空间采用线性算法对样本的非线性特征进行线性分析成为可能。

2 它是基于结果风险最小化理论之上在特征空间中构建最优超平面，使得学习器得到全局最优化，并且在整个样本空间的期望以某个概率满足一定上界。



目的是使支撑向量到类别分类超平面的距离最大



 SVM核函数的选取

1. 线性核	(线性可分时)特征维度高时选取线性核，   易于寻找合适的超平面来进行分类‘

   ​				样本密度高时，选取线性核，易于计算，效率高

2. 多项式核  

3. 高斯核   (线性不可分时)样本容量充足，特征维度低时选取高斯核函数，样本分布密度高



## 支持向量机，SVM，LR区别

支持向量机学习目标为 **最大化分类间隔**，最终可转化为凸二次规划问题解决

LR为参数模型，损失函数为交叉熵，使用梯度下降方法来估计LR参数（对数似然）

SVM为非参数模型，损失函数为Hingeloss



## 监督学习与非监督学习

监督学习  输入数据带有label   KNN

非监督学习	输入数据不带有label    k-means



## 距离计算

余弦距离

$cos = \frac{x_1*x_2+y_1*y_2}{\sqrt{x_2^2+y_1^2}+\sqrt{x_2^2+y_2^2}}$

曼哈顿距离

$dis=|x_1-x_2|*|y_1-y_2|$



## 生成式模型  判别式模型

判别式模型 分割超平面，分割边界灵活且清晰，由数据直接学习决策函数Y=f(X)或条件概率分布P(Y|X)作为预测模型

生成时模型  对后验概率建模，从统计的角度来表示数据的分布情况，由数据学习联合概率分布P(X,Y), 然后由P(Y|X)=P(X,Y)/P(X)求出概率分布P(Y|X)作为预测的模型

预测问题为已知特征x求其类别属性y

两者最大的区别为前者有一个清晰的分割边界，或者说分割超平面，直接求判别边界，即P(y|x)

后者通过后验概率建模，来生成不同类型所占的区域，即其联合概率密度分布P(x,y)

![img](https://pic4.zhimg.com/80/v2-a2e753542fc6384ee351cabdbe6dd523_720w.jpg?source=1940ef5c)

**生成式模型**

+ 朴素贝叶斯
+ K近邻
+ 混合高斯模型
+ 贝叶斯网络
+ 隐马尔科夫模型HMM
+ 马尔科夫随机场
+ 深度信念网络

**判别式模型**

+ 逻辑斯蒂回归模型
+ 线性回归
+ 神经网络
+ 支持向量机
+ 条件随机场
+ CART树

判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。

生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个

**判别模型之所以称为“判别”模型**，是因为其根据X“判别”Y；

**生成模型之所以称为“生成”模型**，是因为其预测的根据是联合概率P(X,Y)，

