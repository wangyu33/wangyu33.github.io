# 决策树

节点分裂根据选取不同节点时所产生的信息熵大小决定，信息熵越大代表不确定性越高，反之则代表确定性越高。通常选取确定性较大的进行节点的分裂。

**信息熵**公式

$H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}$

决策树的构建的目的是希望信息熵能够下降得更快一些，为了使得决策树的信息熵在判断分支中下降得最快，在此引入信息增益的概念：

信息增益）特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验嫡H(D)与特征A给定条件下D的经验条件嫡H(D|A)之差，即

$g(D,A)= H(D)-H(D|A)$

$g(D,A)$越大代表其信息熵下降的越快

具体计算公式如下

$H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}$

$H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}$

$g(D,A)= H(D)-H(D|A)$

$|D|$ 表示其样本容量, $|C_K|$ 为属于类 $|C_K|$ 的样本个数

使用**信息增益**比来限定分支的个数

$g_{R}(D, A)=\frac{g(D, A)}{H(D)}$



决策树算法：

**ID3**

首先从根节点开始，对每个特征进行遍历，假设对当前特征进行分配，计算分配后的子节点的信息熵，选择特征中信息增益最大的特征来划分数据并建立子节点。之后在子节点中递归调用该方法，子子孙孙无穷尽也。

