

# 图网络

## GCN图卷积网络

### GCN出现的动机

+ CNN的关键在于：**局部连接，共享权重和多层使用**

**局部连接**，假设卷积核是5x5大小，对卷积后feature map中的一个像素，这个模板会与以此像素为中心的5x5个像素卷积

**共享权重**，每个feature map使用一个卷积核全图卷积

1. 图是最典型的局部连接结构；
2. 与传统的频谱图理论相比，共享权重降低了计算成本；
3. 多层结构是处理分层模式的关键，它捕获了各种大小的特征。

+ **图嵌入**（Graph Embedding），它旨在学会表示图节点，边缘或低维向量中的子图

之前的图嵌入方法：DeepWalker， node2vec，LINE和TADW

1. 编码器中的节点之间没有共享参数，这导致计算效率低下，因为这意味着参数的数量随节点的数量线性增长；
2. 直接嵌入方法缺乏泛化能力，这意味着它们无法处理动态图或无法泛化为新图。

GNN的目标是为每个节点学习节点的状态嵌入 ![[公式]](https://www.zhihu.com/equation?tex=h_v%5Cin+R%5Es) 的状态，该状态对邻域信息进行编码。状态嵌入![[公式]](https://www.zhihu.com/equation?tex=h_v) 用于产生输出 ![[公式]](https://www.zhihu.com/equation?tex=o_v) ，例如预测节点标签的分布。

## 图的构建

### Spectral Method

使用度矩阵对邻接矩阵进行归一化

1.节点的聚合表征不包含它自己的特征。该表征是相邻节点的特征聚合，因此只有具有自环（self-loop）的节点才会在该聚合中包含自己的特征。

2.度大的节点在其特征表征中将具有较大的值，度小的节点将具有较小的值。这可能会导致梯度消失或梯度爆炸，也会影响随机梯度下降算法（随机梯度下降算法通常被用于训练这类网络，且对每个输入特征的规模或值的范围都很敏感）。，

GCN

邻接矩阵标准化

给邻接矩阵添加自环，然后使用度矩阵对其进行标准化（出度）

GCN在空间域上的操作也可以直观的表现出来，即对于相邻节点，累加所有信息并考虑本节点和相邻节点的度做对称归一化：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924142746528.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2VhZ2xldW5pdmVyc2l0eWV5ZQ==,size_16,color_FFFFFF,t_70#pic_center)

该节点由相邻节点加权获得

之后经过参数矩阵W变换到**隐空间**。这样GCN就和其他基于消息传递的图神经网络统一起来了。GCN的常用表达形式：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924142755638.PNG#pic_center)

缺陷

（a）**无法完成inductive任务，即处理动态图问题。**inductive任务是指：训练阶段与测试阶段需要处理的graph不同。通常是训练阶段只是在子图（subgraph）上进行，测试阶段需要处理未知的顶点。（unseen node）

（b）**处理有向图的瓶颈，不容易实现分配不同的学习权重给不同的neighbor**。这一点在前面的文章中已经讲过了，不再赘述，如有需要可以参考下面的链接。



### Spatial Method

在上述所有频谱方法中，学习的滤波器都取决于拉普拉斯的特征基向量，而后者取决于图的结构。这意味着在特定结构上训练的模型不能直接应用于具有不同结构的图。

空间方法与频谱方法相反，直接在图上定义卷积，在空间上相邻的邻居上进行运算。空间方法的主要挑战是定义大小不同的邻域的卷积运算并保持CNN的局部不变性。

GAT

计算注意力系数

对于顶点 ![[公式]](https://www.zhihu.com/equation?tex=i) , 逐个计算它的邻居和 ![[公式]](https://www.zhihu.com/equation?tex=i) 之间的相似系数:

![[公式]](https://www.zhihu.com/equation?tex=e_%7Bij%7D%3Dg%5Cbig%28Wh_i%5Cmid%5Cmid+Wh_j%5Cbig%29%2C+j%5Cin+%5Cmathcal%7BN%7D_i%5C%5C)

GAT假设相邻节点对中心节点的贡献既不像GraphSage一样相同，也不像GCN那样预先确定。GAT在聚合节点的邻居信息的时候**使用注意力机制确定每个邻居节点对中心节点的重要性**，也就是权重。定义图卷积操作如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924132638443.PNG#pic_center)

图注意力层，在文中描述为"Graph Attentional Layer"。我在下文中会简称为GAL。我们先理解何为注意力(attention)，我们继续看这张图：

![img](https://pic3.zhimg.com/v2-8c10256a73c3f7599f8efa17251a6582_b.jpg)

对于节点3，它的邻接节点只有节点2和节点4，但不代表这两个节点对节点3具有一样的重要性。这个“重要性”可以进行量化，更可以通过网络训练得出。这个“重要性”，在文中叫attention，可以通过训练得到。**这便是GAT的核心创新点了**。

这个attention是不满足对称性(后面会证明)，即节点2对节点3的attention与节点3对节点2的attention是不一样的，把每个连接(edge)当成桥的话，这个attention类似桥的宽度。当然，简化版的GAT中可以使这个attention变得对称。整个论文的数学讨论就在于如何训练attention，以及将attention融入图神经网络中。

我们直接看一个核心公式：

![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D%3Dsoftmax%28%5Csigma%28%7B%5Cvec%7Ba%7D%7D%5ET%5BW%7B%5Cvec%7Bh_i%7D%7D+%7C%7CW%7B%5Cvec%7Bh_j%7D%7D+%5D%29%29%5Ctag%7B2%7D) 

这个公式和论文中的公式(2)和公式(3)是对应的，我只是转化成了用更直观的形式。![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D) 表示节点i和节点j之间的attention系数，**咱们由内向外看看这个公式**:

首先权重矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W) ，是一个 ![[公式]](https://www.zhihu.com/equation?tex=F%C3%97F%27) 尺寸的矩阵。 ![[公式]](https://www.zhihu.com/equation?tex=F) 表示输入节点特征的维数，而 ![[公式]](https://www.zhihu.com/equation?tex=F%27) 表示该层输出节点的维数。而其中的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvec%7Bh_i%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvec%7Bh_j%7D) 表示，节点 ![[公式]](https://www.zhihu.com/equation?tex=i) 和节点 ![[公式]](https://www.zhihu.com/equation?tex=j) 的节点特征，如果这层GAL为输入层，那么节点特征直接就是图的原节点特征 ![[公式]](https://www.zhihu.com/equation?tex=x) 。注意这里变量名为什么用 ![[公式]](https://www.zhihu.com/equation?tex=h) 而不用 ![[公式]](https://www.zhihu.com/equation?tex=x) ，其想表达的是这个节点特征会随着层的堆叠而改变，所以用 ![[公式]](https://www.zhihu.com/equation?tex=h) 来表示**隐藏层特征hidden feature**。通过前面的描述，应该不难看出 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvec%7Bh_i%7D) 的维度是 ![[公式]](https://www.zhihu.com/equation?tex=1%C3%97F) 吧。通过线代的知识，我们轻易知道 ![[公式]](https://www.zhihu.com/equation?tex=W%5Cvec%7Bh_i%7D) 的维度为 ![[公式]](https://www.zhihu.com/equation?tex=1%C3%97F%27) 。

重点是那个双竖线"||"，这个符号在文中代表**concatenate，**表示张量的粘合。张量的粘合就是，[[1， 2], [3，4]]粘合[[5, 6], [7, 8]]变成[[1， 2], [3，4]，[5, 6], [7, 8]]。这个栗子很形象吧，但是不同维度进行concatenate的效果是不一样的，详情请看我另一篇文章《[tf.concat详解](https://blog.csdn.net/leviopku/article/details/82380118)》。

通过concat，我们把两个 ![[公式]](https://www.zhihu.com/equation?tex=1%C3%97F%27) 的张量粘合成了 ![[公式]](https://www.zhihu.com/equation?tex=+1%C3%972F%27+) 的大张量。然后乘以一个 ![[公式]](https://www.zhihu.com/equation?tex=+2F%27%C3%971+) 的**attention kernel:** ![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cvec%7Ba%7D%7D%5ET) 。这样不就可以得到1个数么，这个数就是未加工的attention系数。用图来解释这个过程会非常直观，这里取 ![[公式]](https://www.zhihu.com/equation?tex=F%E2%80%99%3D4) 

此外，该层也利用多头注意力以稳定学习过程。它应用 K 个独立的注意力机制来计算隐藏状态，然后将其特征连接起来（或计算平均值），从而得到以下两种输出表示形式：

![img](https://pic1.zhimg.com/80/v2-9e69c37dcb4d7df577ad7c83f81a3360_720w.jpg)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D%5Ek) 是第 k 个注意力头归一化的注意力系数， ![[公式]](https://www.zhihu.com/equation?tex=%7C%7C) 表示拼接操作。模型细节如下图所示：

聚合函数

**根据计算好的注意力系数，把特征加权求和（aggregate）一下。**

![[公式]](https://www.zhihu.com/equation?tex=h_i%5E%7B%27%7D%3D%5Csigma%5Cleft%28+%5Csum_%7Bj%5Cin+%5Cmathcal%7BN%7D_i%7D%7B%5Calpha_%7Bij%7DW%7Dh_j+%5Cright%29+%5Cqquad+%283%29)

![[公式]](https://www.zhihu.com/equation?tex=h_i%5E%7B%27%7D) 就是GAT输出的对于每个顶点 ![[公式]](https://www.zhihu.com/equation?tex=i) 的新特征（融合了邻域信息）， ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%5Cleft%28+%5Ccdot+%5Cright%29) 是激活函数。



GraphSage

**GraphSAGE**。该模型是一个泛化的inductive框架，通过采样和聚合邻居节点的特征来产生节点的嵌入。传播过程为：

![[公式]](https://www.zhihu.com/equation?tex=h%5Et_%7BN_v%7D%3DAGGREGATE_t%28%5C%7Bh_u%5E%7Bt-1%7D%2C%5Cforall+u%5Cin+N_v%5C%7D%29%2C%5C%5Ch%5Et_v%3D%5Csigma%28W%5Et%5Ccdot+%5Bh_v%5E%7Bt-1%7D%7C%7Ch_%7BN_v%7D%5Et%5D%29.%2820%29)

其中， ![[公式]](https://www.zhihu.com/equation?tex=W%5Et) 是第 ![[公式]](https://www.zhihu.com/equation?tex=t) 层的参数。

然而，GraphSAGE并不使用所有的相邻节点，而是随机采样固定数量的相邻节点，AGGREGATE步骤可以有多种形式，包括：

1. **平均聚合**（Mean aggregator）。可以看作是近似版本的transductive GCN卷积操作，inductive GCN的变体写作： ![[公式]](https://www.zhihu.com/equation?tex=h%5Et_v%3D%5Csigma%28W%5Ccdot+MEAN%28%5C%7Bh_v%5E%7Bt-1%7D%5C%7D%5Ccup+%5C%7Bh_u%5E%7Bt-1%7D%2C%5Cforall+u%5Cin+N_v%5C%7D%29%29.%2821%29) 这个聚合方法和其他方法不同之处在于不像（20）中那样进行拼接操作，可以被看作是一种残差连接（skip connection），所以效果更好。
2. **LSTM聚合**（LSTM aggregator）。基于LSTM实现，具有更高的表达能力，但是由于建模序列数据，所以不同的排列会造成不同的结果。*可以通过修改LSTM的实现来对无序的相邻节点集合作处理。*
3. 池化聚合（Pooling aggregator）。将邻节点的嵌入通过一个全连接层并最大池化： ![[公式]](https://www.zhihu.com/equation?tex=h_v%5Et%3Dmax%28%5C%7B%5Csigma%28W_%7Bpool%7Dh_u%5E%7Bt-1%7D%2Bb%29%2C%5Cforall+u%5Cin+N_v%5C%7D%29.%2822%29) ，这里的池化也可以用任意对称的计算替代。



### 图的类型

有向图

第一个变种，有向图，在边上增加了方向信息。

同构图

在图里面，节点的类型和边的类型只有一种的图，举个例子，像社交网络中只存在一种节点类型，用户节点和一种边的类型，用户-用户之间的连边。

异构图

异构图指的是图中存在不同类型的节点和边（节点和边至少有一个具有多种类型），常见于知识图谱的场景。最简单的处理异构信息的方式是使用独热编码类型信息并拼接在节点原有表示上。

动态图

图中的节点信息、节点存在与否动态变化，这要求模型泛化能力极强并且拥有很好的灵活性。