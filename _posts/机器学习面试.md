# 机器学习面试

#### 为什么要进行归一化，有哪些归一化方法，那些模型需要归一化，那些不需要，为什么

为什么

+ 解决数据量纲不一致导致的特征权重大小差距很大

+ 加速模型训练收敛

有哪些

最大最小归一化：

$$X_{\text {norm }}=\frac{X-X_{min}}{X_{\max }-X_{\min }}$$

零均值归一化：

$X_{norm}=\frac{X-X_{mean}} {X_{std}}$

svm，深度学习需要

需要解决量纲问题，加速收敛

树模型不需要

树模型在训练的过程中通过信息增益、信息增益率、gini系数来进行树的生长，不涉及梯度下降过程

#### Batch Normalization层原理，作用

利用每一个minibatch的均值和方差对每个输入层进行零均值归一化

减轻梯度爆炸带来的影响

加速模型收敛

#### 比较LR和GBDT，什么情景下GBDT不如LR

LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；

**高维稀疏特征的场景下，LR的效果一般会比GBDT好\**带正则化的线性模型比较不容易对稀疏特征过拟合。\****

#### 常见的决策树算法有哪些？请描述它们在进行树的生成过程中，具体的特征选择算法，以及它们的对比？

ID3，C4.5，CART分类回归树

分别采用信息增益、信息增益率、基尼系数进行特征选择。

信息增益偏向取值较多的特征；

信息增益比是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征；

基尼系数代表了特征的不纯度，值越小则区分度越高，在保留熵信息的基础上，大大简化了模型计算量。

#### 深度学习中，常用的层有哪些，各有什么作用和特点？

1. 全连接层：最基础的神经网络层，每个节点接收上一层的全部节点作为输入，计算代价较大。
2. 卷积层:由多个卷积核生成而来，采用了局部视野、权值共享的思想。
3. 池化层:缩减输入数据规模，增强鲁棒性
4. 激活层:传统是softmax，带来梯度消失于梯度爆炸问题，增加ReLU激活函数后，更多层次的深度学习才可进行。
5. RNN层：考虑时间、顺序特征，每个神经元的当前状态也被其上一个时间的状态所影响。
6. LSTM层：在RNN的基础上增加输入门、遗忘门、输出门的概念、可以支持更远距离的信息保留。
7. GRU层：在LSTM层上作了约束，只有更新门与重置门，也因为参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好
8. Dropout层：是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃
9. Batch Normalization层：批规范化，在每次SGD时，通过mini-batch来对相应的activation做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1. 而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入解决了反向传播过程中的梯度问题（梯度消失和爆炸），同时使得不同scale的 整体更新步调更一致。



#### 机器学习中常见的最优化方法有哪些，请简述它们的原理、缺陷以及改进。

主要考察梯度下降法与牛顿法的掌握

1. 传统梯度下降是指，在给定待优化的模型参数 和目标函数 后，算法通过沿梯度 的相反方向更新 来最小化 。学习率 决定了每一时刻的更新步长。缺点在于收敛速度慢，可能在鞍点处震荡。并且，如何合理的选择学习率是 传统梯度下降方法的一大难点。
2. 可以为其引入动量 Momentum，加速 梯度下降法在正确方向的下降并抑制震荡。成为SGD-M
3. 在目标函数有增高趋势之前，减缓更新速率。成为Nesterov Accelerated Gradient
4. 使用自适应步长方法，对于此前频繁更新过的参数，其二阶动量的对应分量较大，学习率就较小。这一方法在稀疏数据的场景下表现很好。成为Adagrad

1. 牛顿法引入了二阶导数来调整学习率状态。具体的说:

牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。

#### 常见的聚类算法有哪些，请描述它们的原理。

1.基于划分
给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K<N。
特点： 计算量大。很适合发现中小规模的数据库中小规模的数据库中的球状簇。
算法： K-MEANS算法、K-MEDOIDS算法、CLARANS算法

2.基于层次
对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案。
特点： 较小的计算开销。然而这种技术不能更正错误的决定。
算法： BIRCH算法、CURE算法、CHAMELEON算法

3.基于密度
只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去。
特点： 能克服基于距离的算法只能发现“类圆形”的聚类的缺点。
算法： DBSCAN算法、OPTICS算法、DENCLUE算法

4.基于网格
将数据空间划分成为有限个单元（cell）的网格结构,所有的处理都是以单个的单元为对象的。
特点： 处理速度很快，通常这是与目标数据库中记录的个数无关的，只与把数据空间分为多少个单元有关。
算法： STING算法、CLIQUE算法、WAVE-CLUSTER算法

#### 请简介word2vec的原理，以及在实现过程中会涉及到的参数与参数解释。

word2vec就是用一个一层的神经网络(CBOW的本质)把one-hot形式的词向量映射为分布式形式的词向量，为了加快训练速度，用了Hierarchical softmax，negative sampling 等trick。训练的主要目标是获得分布式词向量，而不是神经网络的预测模型。word2vec的训练过程是有监督的神经网络学习，但是得到的结果居然是无监督的clustering的效果，就是获取词向量。分布式词向量隐含了词语的信息，分布式词向量之间的夹角可以表示词语之间的相关性，因此用作特征值比直接用词本身更方便。word2vec还规避了两大问题：词语的次序和热门词语的降权处理。与潜在语义分析（Latent Semantic Index, LSI）、潜在狄立克雷分配（Latent Dirichlet Allocation，LDA）的经典过程相比，Word2vec利用了词的上下文，语义信息更加地丰富。



![img](https://uploadfiles.nowcoder.com/files/20200215/323193_1581696461293_1825ee333e514ece990c91a60fab8e16.png)

-train 训练数据 
-output 结果输入文件，即每个词的向量 
-cbow 是否使用cbow模型，0表示使用skip-gram模型，1表示使用cbow模型，默认情况下是skip-gram模型，cbow模型快一些，skip-gram模型效果好一些 
-size 表示输出的词向量维数 
-window 为训练的窗口大小，8表示每个词考虑前8个词与后8个词（实际代码中还有一个随机选窗口的过程，窗口大小<=5) 
-negative 表示是否使用NEG随机负采样方法，0表示不使用，其它的值表示采用发负样例是多少 
-hs 是否使用HS （Hierarchical Softmax）方法，0表示不使用，1表示使用 
-sample 表示 采样的阈值，如果一个词在训练样本中出现的频率越大，那么就越会被采样 
-binary 表示输出的结果文件是否采用二进制存储，0表示不使用（即普通的文本存储，可以打开查看），1表示使用，即vectors.bin的存储类型
除了上面所讲的参数，还有： 
-alpha 表示 学习速率 
-min-count 表示设置最低频率，默认为5，如果一个词语在文档中出现的次数小于该阈值，那么该词就会被舍弃 
-classes 表示词聚类簇的个数，从相关源码中可以得出该聚类是采用k-means

#### **请简述生成式模型与判别式模型的区别，并简单列举几个分别属于生成式模型与判别式模型的常见算法**

生成式模型(Generative Model)与判别式模型(Discrimitive Model)是分类器常遇到的概念，它们的区别在于：
对于输入x，类别标签y：
产生式模型估计它们的联合概率分布P(x,y)
判别式模型估计条件概率分布P(y|x)

![img](https://uploadfiles.nowcoder.com/files/20200215/323193_1581696428632_01eb45584af24d949dc140f2bec359e1.png)

生成式模型

- 判别式分析
- 朴素贝叶斯
- K近邻(KNN)
- 混合高斯模型
- 隐马尔科夫模型(HMM)
- 贝叶斯网络
- Sigmoid Belief Networks
- 马尔科夫随机场(Markov Random Fields)
- 深度信念网络(DBN)

判别式模型

- 线性回归(Linear Regression)
- 逻辑斯蒂回归(Logistic Regression)
- 神经网络(NN)
- 支持向量机(SVM)
- 高斯过程(Gaussian Process)
- 条件随机场(CRF)
- CART(Classification and Regression Tree)

#### 机器学习中几乎都可以看到损失函数后面会添加一个额外项，一般选用L1正则化和L2正则化，请简述他们的原理与之间的区别。

L0正则化的值是模型参数中非零参数的个数，L1范数是指向量中各个元素绝对值之和，L2正则化标识各个参数的平方的和的开方值。

L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，但这些特征都会接近于0。在所有特征中只有少数特征起重要作用的情况下，选择L1比较合适，因为它能自动选择特征。而如果所有特征中，大部分特征都能起作用，而且起的作用很平均，那么使用L2也许更合适。L1不仅可以作为正则化手段，其在特征选择时候非常有用，而L2就只是一种规则化而已。

#### 逻辑回归，相比于线性回归，有何异同？请简述逻辑回归的原理

1.逻辑回归解决的是分类问题，线性回归解决的是回归问题，这是两者最本质的区别

2.逻辑回归中因变量是离散的，而线性回归中因变量是连续的这是两者最大的区别

3在自变量和超参数确定的情况下逻辑回归可看作广义的线性模型在因变量下服从二元分布的一个特殊情况

4.使用最小二乘法求解线性回归时我们认为因变量服从正态分布

相同之处：
1.二者在求解超参数的过程中都使用梯度下降的方法

2.二者都使用了极大似然估计对训练样本进行建模

#### 有哪些评估机器学习模型效果的指标？请简述他们的计算方法。

precision：TP/(TP+FP)	真正类/（真正类+假正类）	预测为正类被分对的概率

recall：TP/(TP+FN)			真正类/（真正类+假负类）	灵敏度

f1-score：2* precision*recall/(precision + recall)

accuracy：分类正确的样本数/总样本数

回归

mae：1/n*sum(abs(y_i-yhat_i))

mse：1/n*sum((y_i-yhat_i)^2)

**AUC(Area under the Curve(Receiver Operating Characteristic, ROC))**

 AUC的全称是Area under the Curve，即曲线下的面积，这条曲线便是ROC曲线，全称为the Receiver Operating Characteristic曲线，ROC曲线描述分类器的True Positive Rate（TPR，分类器分类正确的正样本个数占总正样本个数的比例）与False Positive Rate（FPR，分类器分类错误的负样本个数占总负样本个数的比例）之间的变化关系。



#### 对参数w的L1正则等价于w的先验服从拉普拉斯分布

l2服从高斯分布



### softmax与sigmoid函数

sigmoid函数与逻辑斯特函数类似，用于二分类

$f(x) =  \frac{1}{1+e^{-x}}$

softmax函数多用于多分类之中，将输出值映射到0-1，且累加和为1

$S_i=\frac{e^i}{\sum_{j}e^{i}}$



#### Relu反向传播函数

$y=Wx+b$

$g=f(y) f(y) = Relu(y)$

$$\begin{equation}
\frac{dg}{dy}=\left\{
\begin{aligned}
0 & , & y<0, \\
1 & , & y\geq0.
\end{aligned}
\right.
\end{equation}$$

$$\begin{equation}
\frac{dg}{dx}=\frac{dg}{dy}\frac{dy}{dx}=\left\{
\begin{aligned}
0 & , & y<0, \\
w & , & y\geq0.
\end{aligned}
\right.
\end{equation}$$



#### 常用机器学习算法的损失函数

LR损失函数为交叉熵

SVM损失函数为带hinge的L2范数损失函数



#### KNN为有监督	Kmeans为无监督





### GBDT家族

传统GBDT使用Cart树来作为基分类器

其损失函数



XGBoost相较于GBDT的提升：

1：对函数由一阶导变为使用一二阶导，使用二阶偏导可以使得损失下降的速度更快，下降方向更清晰，速度更快，**泰勒的本质是尽量去模仿一个函数，二阶泰勒展开已经足以近似大量损失函数了**，这样对于不同的损失函数不用每次都推导一番，重写训练代码；

2：添加了正则项，防止过拟合；

3：支持特征粒度预排序，一次排序，全局使用，减少排序时间消耗（需存储，空间换时间）；

4：支持直方图近似算法，根据数据分布确定划分点，将连续数据进行分桶，统计桶内的一二阶导数，进行存储，不再存储所有数据的一二阶导数，减少存储所需要的空间；

5：支持加权分位数算法，在大数据寻找有效分位点时很有效；

6：支持缺失值处理，自动学习分类方向；

7：优化cache命中（针对预排序算法）；

8：支持列采样（类似随机森林）



LightGBM相较于XGBoost的提升：

1：支持类别特征，无需one-hot编码；

2：采用叶节点分裂而非层分裂，学习精度更高；

3：**GOSS（基于梯度的单边采样）**，对海量学习数据，根据其梯度，筛除绝大部分的小梯度样本（几乎无更新作用），保持精度的同时加快速度；

4：EFB（独立特征合并），针对海量稀疏数据，根据数据间的冲突度（如cos夹角，0101和1010的冲突很小，因为非零位不相同，非零位不相同的占比越高，冲突度越少），对冲突度小的特征进行合并，变稀疏矩阵为稠密矩阵，减少特征维度；

5：在直方图加速算法中，支持直方图减法，对下一层样本的直方图绘制，减少时间（右孩子直方图=父节点直方图-左孩子直方图）。



### 线性回归

特征

+ 线性

  - 属性是线性的

    属性非线性改进：特征变换、多项式回归

  - 系数是线性的

    系数非线性改进： 神经网络

  - 全局是线性的

    全局非线性改进： 激活函数

+ 全局性

  ​	改进 线性样条回归、决策树

+ 数据未加工

  数据加工处理	PCA、流形





### 深度学习优化器

SGD (Stochastic Gradient Descent)随机梯度下降

朴素 SGD (Stochastic Gradient Descent) 最为简单，没有动量的概念，即

![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Ceta+g_t)

![[公式]](https://www.zhihu.com/equation?tex=v_t+%3D+I%5E2)

![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon+%3D+0)

这时，更新步骤就是最简单的

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%2B1%7D%3D+%5Ctheta_t+-+%5Ceta+g_t)

SGD 的缺点在于收敛速度慢，可能在鞍点处震荡。并且，如何合理的选择学习率是 SGD 的一大难点。

为了避免SGD陷入暗点之中，引入了动量的概念

![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cgamma+m_%7Bt-1%7D+%2B+%5Ceta+g_t)

SGD-M 在原步长之上，增加了与上一时刻步长相关的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+m_%7Bt-1%7D) ，![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 通常取 0.9 左右。这意味着参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关。

Adagrad

SGD、SGD-M 和 NAG 均是以相同的学习率去更新 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的各个分量。而深度学习模型中往往涉及大量的参数，不同参数的更新频率往往有所区别。对于更新不频繁的参数（典型例子：更新 word embedding 中的低频词），我们希望单次步长更大，多学习一些知识；对于更新频繁的参数，我们则希望步长较小，使得学习到的参数更稳定，不至于被单个样本影响太多。

Adagrad[4] 算法即可达到此效果。其引入了二阶动量：



RMSprop

在 Adagrad 中， ![[公式]](https://www.zhihu.com/equation?tex=v_t) 是单调递增的，使得学习率逐渐递减至 0，可能导致训练过程提前结束。为了改进这一缺点，可以考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间窗口内的下降梯度。根据此思想有了 RMSprop[5]。记 ![[公式]](https://www.zhihu.com/equation?tex=g_t+%5Codot+g_t) 为 ![[公式]](https://www.zhihu.com/equation?tex=g_t%5E2) ，有

![[公式]](https://www.zhihu.com/equation?tex=v_t+%3D+%5Cgamma+v_%7Bt-1%7D+%2B+%281-%5Cgamma%29+%5Ccdot+%5Ctext%7Bdiag%7D%28g_t%5E2%29) 

其二阶动量采用指数移动平均公式计算，这样即可避免二阶动量持续累积的问题。和 SGD-M 中的参数类似，![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 通常取 0.9 左右。

Adam

Adam[6] 可以认为是 RMSprop 和 Momentum 的结合。和 RMSprop 对二阶动量使用指数移动平均类似，Adam 中对一阶动量也是用指数移动平均计算。

![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Ceta%5B+%5Cbeta_1+m_%7Bt-1%7D+%2B+%281+-+%5Cbeta_1%29g_t+%5D) 

![[公式]](https://www.zhihu.com/equation?tex=v_t+%3D+%5Cbeta_2+v_%7Bt-1%7D+%2B+%281-%5Cbeta_2%29+%5Ccdot+%5Ctext%7Bdiag%7D%28g_t%5E2%29) 

其中，初值

![[公式]](https://www.zhihu.com/equation?tex=m_0+%3D+0) 

![[公式]](https://www.zhihu.com/equation?tex=v_0+%3D+0) 

注意到，在迭代初始阶段，![[公式]](https://www.zhihu.com/equation?tex=m_t) 和 ![[公式]](https://www.zhihu.com/equation?tex=v_t) 有一个向初值的偏移（过多的偏向了 0）。因此，可以对一阶和二阶动量做偏置校正 (bias correction)，

![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bm%7D_t+%3D+%5Cfrac%7Bm_t%7D%7B1-%5Cbeta_1%5Et%7D) 

![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bv%7D_t+%3D+%5Cfrac%7Bv_t%7D%7B1-%5Cbeta_2%5Et%7D) 

再进行更新，

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_t+-+%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Chat%7Bv%7D_t%7D+%2B+%5Cepsilon+%7D+%5Chat%7Bm%7D_t) 

可以保证迭代较为平稳。



**为什么引入二阶动量**

深度学习中不同的参数的更新频率往往不同，对于更新不频繁的参数希望单次步长更长，更新频繁的参数步长变小，**相当于使用二阶动量来修改学习率**

**动量梯度下降法的运行速度几乎总是快于标准的梯度下降算法。**简而言之，其基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新你的权重。



### 线性分类

硬分类    y∈{0,1}

+ fisher判别
+ 感知机

软分类    y∈[0,1]

+ 生成式模型    高斯判别函数    使用MAP最大后验估计来求
+ 判别式模型    逻辑回归



Fisher 线性判别

寻找一个投影平面使得类内间距小，类间间距大



### LR 损失函数用交叉熵原因

LR的极大似然估计的参数形式是以交叉熵的形式呈现，可以说最小化交叉熵的本质就是对数似然函数的最大化。





### 如何理解交叉熵

交叉熵本质上可以理解为求解结果与目标结果的相关系数，交叉熵的值减去原目标结果的信息熵即为求解结果与目标结果的KL散度，即求解结果与目标结果的相对熵，熵的变化量





### PCA

+ 原始空间的重构，将向量空间中的基向量转变为无关基向量

+ 目标：

  + 最大投影方差
  + 最小重构距离

  中心化

  $x_i = x_i - \bar{x}$

  各个方向上减去均值再除以向量模长

  $J= \sum_{i=1}((x_i-\bar{x})^Tu_1)^2$

  

通过求取协方差的特征向量和特征根

$Su=\lambda u$ 其中S为对称矩阵

重构$\hat{x_i} = \sum_{i=k}(x_iu_k)^Tu_k$





### SVM

SVM有三宝：间隔、对偶核技巧

SVM：hard_margin SVM

​			soft_margin SVM

​			kernel SVM

f(w) = sign(wx+b) 判别模型

硬间隔：将所有样本点都正确分类

margin(wb)表示所有样本点离分割平面最近的距离

margin(wb) = min distance(wb,x_i)

= $min_{w_ib_i,x_i} {\frac{1}{||w||}|w^Tx_i+b|}$

软间隔：允许一点点分类错误





对抗生成模型

**对抗训练，相当于是加了一层正则化，给神经网络的随机梯度优化限制了一个李普希茨的约束**





### 核函数

线性核、多项式核、高斯核。

特征维数高选择线性核

样本数量可观、特征少选择高斯核（非线性核）

样本数量非常多选择线性核（避免造成庞大的计算量）



**当样本的特征很多且维数很高时可考虑用SVM的线性核函数**。当样本的数量较多,特征较少时,一般手动进行特征的组合再使用SVM的线性核函数。当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用高斯核函数,因为高斯核函数为一种局部性较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数。



SVM损失函数

$J(\theta)=\frac{1}{2}||\theta||^2+C\sum_{i}max(0,1-y_i(\theta^Tx_i+b))$



### 常见的距离

欧氏距离	  L2

曼哈顿距离  L1

Cosine